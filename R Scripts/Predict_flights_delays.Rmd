---
title: "Predicting flight delays"
author: "BA-810 Team #2"
date: "2/27/2021"
output: html_document
---
## Introduction

Flying doesn't always go smoothly. Many people have had some horror stories with weird delays but others haven't experienced this (Lucky them!). This brings us to the following question:  Wouldn't it be nice to know how much your flight will probably be delayed?

That what this project will attempt to do, specifically for all airlines in Los Angeles, Boston and Atlanta airport. This will be a regression problem where we will try to predict delay time in number of minutes. 

## Getting the data

We gathered our flight data from the US Department of Transportation for March 2019 and 2020. We chose the following features:

* DAY_OF_MONTH
* YEAR
* DAY_OF_WEEK
* DEP_DELAY
* CRS_ARR_TIME
* CRS_ELAPSED_TIME
* CRS_DEP_HOUR 
* AIRLINE
* humidity
* precipMM
* pressure
* tempC
* visibility
* windspeedKmph 

The weather variables were retrieved using the World Weather API. We created a new column called "Total_Cancellations" which means the number of cancellations the day before in the specific airport and airline. 

## Load libraries
```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(scales)
library(dplyr)
library(glmnet)
library(rpart) 
library(rpart.plot)
library(randomForest)
library(tidyverse)
library(caret)
library(ipred)# for fitting bagged decision trees
library(gbm)
library(boot)

theme_set(theme_bw())
```

## Load Dataset 

The "df" contains observations for the three airports. The other three are subset data tables for each airport respectively.

```{r}
df <- fread('flights.csv')
df.la <- df[ORIGIN == 'LAX', ]
df.bos <- df[ORIGIN == 'BOS', ]
df.atl <- df[ORIGIN == 'ATL', ]
summary(df)
```


## Exploratory data analysis


```{r}
delayed <- df[DEP_DELAY > 0, .(number_delays = .N), by = ORIGIN]
not_delayed <- df[DEP_DELAY <= 0,.(number_on_time = .N), by= ORIGIN]
total <- merge(delayed, not_delayed, by='ORIGIN')
total$ORIGIN <- as.factor(total$ORIGIN)
dat_long <- total %>%
  gather("Stat", "Value", -ORIGIN)
setDT(dat_long)
ggplot(dat_long, aes(x = ORIGIN, y = Value, fill = Stat)) +
  geom_col(position = "dodge") + ylab('Number of flights')  + ggtitle("Delay vs. On-time flights")
```
```{r}
flights_by_year <-df[,.(Number_flights = .N), by =.(ORIGIN, YEAR)]
flights_by_year$YEAR <- as.factor(flights_by_year$YEAR)

ggplot(flights_by_year, aes(x = ORIGIN, y = Number_flights, fill =YEAR)) +
  geom_col(position = "dodge") + ylab('Number of flights') + ggtitle("Number of flights - 2019 vs. 2020")

```



```{r}
weekly_delay <-df[,.(mean_delay = mean(DEP_DELAY)), by =.(DAY_OF_WEEK = df$DAY_OF_WEEK, ORIGIN = df$ORIGIN)]
ggplot(weekly_delay,aes(x = DAY_OF_WEEK, y = mean_delay, color=ORIGIN)) + geom_point() + geom_line()  + ylab('Average Delay') + xlab('Day of the week') + ggtitle("Averga delay in days of the week")
```


```{r}
day_month_delay <-  df[,.(mean_delay = mean(DEP_DELAY)), by =.(DAY_OF_MONTH = DAY_OF_MONTH, ORIGIN = ORIGIN)]
ggplot(day_month_delay,aes(x = DAY_OF_MONTH, y = mean_delay, color = ORIGIN)) + geom_point() + geom_line() + ylab('Average Delay') + xlab('Day of the month') + ggtitle("Averga delay in days of the month")

```


```{r}
distance_delay_weekly <- df.la[,.(delay = mean(DEP_DELAY), dist = mean(DISTANCE)), by ='DAY_OF_WEEK']
ggplot(distance_delay_weekly,aes(x = dist, y = delay)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Los Angeles - Average Departure and distance by day of the week") + ylab("Average Departure Delay") + xlab("Average Flight Distance")

distance_delay_airline <- df.la[,.( delay = mean(DEP_DELAY), dist = mean(DISTANCE)), by ='AIRLINE']
ggplot(distance_delay_airline,aes(x = dist, y = delay)) + geom_point() + geom_smooth(method = "lm") + ggtitle("Los Angeles - Average Departure and flight distance by airline") + ylab("Average Departure Delay") + xlab("Average Flight Distance")

```

## Models for each airport

In this section, we present the best models for each airport location.

### Los Angeles Airport


#### Splitting the datasets
```{r}
set.seed(123)
smp_size <- floor(.80 * nrow(df.la))
train_index <- sample(nrow(df.la), smp_size) # assign 80% of the data to train
df.test <- df.la[-train_index,] ##not the one in train_index
df.train <-df.la[train_index,] 
```

#### Naive Regression

Started doing the Naive Regression (Baseline) to have an MSE to compare to. 

```{r}
set.seed(123)
y.test.b <- df.test$DEP_DELAY
df_la_y <- mean(df.la$DEP_DELAY)

mse_baseline <- mean((y.test.b - df_la_y)^2)
rmse_baseline <- sqrt(mse_baseline)
rmse_baseline 
```

Our goal is to apply more advanced ML methods to obtain a lower RMSE.

#### Linear Regression

##### Validation set approach

Fit the model and make predictions using the two data sets created earlier. 

```{r}
y.train.lm <- df.train$DEP_DELAY
y.test.lm <- df.test$DEP_DELAY

f1 <- as.formula(DEP_DELAY ~ DAY_OF_MONTH + YEAR + DAY_OF_WEEK + AIRLINE 
                 + CRS_DEP_HOUR  + CRS_ARR_TIME + humidity + precipMM + pressure 
                 + tempC + visibility + windspeedKmph)

fit.lm1 <- lm(f1,df.train) 

#Let's compute an MSE on the training data
yhat.train.lm1 <- predict(fit.lm1)
mse.train.lm1 <- mean((y.train.lm - yhat.train.lm1)^2)
mse.train.lm1

#Let's compute an MSE on the test data
yhat.test.lm1 <- predict(fit.lm1, df.test)
mse.test.lm1 <- mean((y.test.lm - yhat.test.lm1)^2)
mse.test.lm1

rmse_lm1 <- sqrt(mse.test.lm1)
rmse_lm1

```

##### K-folds cross validation

Considered the LOOCV method but it was too much computational cost. We decided to do the K-folds approach. 

```{r}
train.control <- trainControl(method = "cv", number = 10)
#train.control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
model <- train(f1, data = df.la, method = "lm", trControl = train.control) # Train the model
print(model) # Summarize the results
```

Got a lower RMSE using K-means fold cross validation. However, we decided that we need a more flexible model to yield a better MSE. 


#### Ridge Regression

Ridge regression shrinks coefficients towards zero. We used glmnet command to shrink coefficients towards zero. We passed the predictors matrix as parameters and used alpha=0 to invoke ridge regression. 

```{r}
dd <- copy(df.la)
dd[, test:=0] #Adds a new column with value 0
dd[sample(nrow(dd),5000), test:=1] #Take 5k random rows and assign it to test
dd.test <- dd[test==1]
dd.train <- dd[test==0] #Around 33K for training

#assign our response variable (target)
y.train.r <- dd.train$DEP_DELAY
y.test.r <- dd.test$DEP_DELAY

x1.train <- model.matrix(f1, dd.train)[,-1]
x1.test <- model.matrix(f1,dd.test)[,-1]

fit.ridge <- cv.glmnet(x1.train, y.train.r, alpha = 0, nfolds = 10)

##Test the MSE in training data
yhat.train.ridge <- predict(fit.ridge, x1.train, s = fit.ridge$lambda.min)
mse.train.ridge <- mean((y.train.r - yhat.train.ridge)^2)
mse.train.ridge

##Test the MSE test
yhat.test.ridge <- predict(fit.ridge, x1.test, alpha = 0, s = fit.ridge$lambda.min)
mse.test.ridge <-  mean((y.test.r - yhat.test.ridge)^2)
mse.test.ridge 

rsme_test_ridge <- sqrt(mse.test.ridge)
rsme_test_ridge 

```

Ridge Regression gives the lowest MSE for Los Angeles airport with +- `r rsme_test_ridge`. This make sense because we have high variance in the data set mainly because of multiple outliers representing longer/weird delays.


#### Lasso Regression

The lasso is like ridge regression â€“ but instead of shrinking coefficients towards zero,it tries to set as many as it can to zero.

```{r}
dd.lasso <- copy(df.la)
dd.lasso[, test:=0] 
dd.lasso[sample(nrow(dd.lasso),5000), test:=1]
dd.test.l <- dd.lasso[test==1]
dd.train.l <- dd.lasso[test==0] #Around 33K for training

y.train.lasso <- dd.train.l$DEP_DELAY
y.test.lasso <- dd.test.l$DEP_DELAY

x2.train <- model.matrix(f1, dd.train.l)[,-1]
x2.test <- model.matrix(f1,dd.test.l)[,-1]

fit.lasso <- cv.glmnet(x2.train, y.train.lasso, alpha = 1, nfolds = 10)

##Test the MSE in training data
yhat.train.lasso <- predict(fit.lasso, x2.train, s = fit.lasso$lambda.min)
mse.train.lasso <- mean((y.train.lasso - yhat.train.lasso)^2)
mse.train.lasso

##Test the MSE test
yhat.test.lasso <- predict(fit.lasso, x2.test, alpha = 0, s = fit.lasso$lambda.min)
mse.test.lasso <-  mean((y.test.lasso - yhat.test.ridge)^2)
mse.test.lasso 

rmse_lasso <- sqrt(mse.test.lasso)
rmse_lasso
```


#### Decision Tree Model

```{r}
smp_size <- floor(.75 * nrow(df.la))
train_index <- sample(nrow(df.la), smp_size) 
df.test.dt <- df.la[-train_index,] ##not the one in train_index
df.train.dt <-df.la[train_index,] 

y.train.dt <- df.train.dt$DEP_DELAY
y.test.dt <- df.test.dt$DEP_DELAY

f5 <- as.formula(DEP_DELAY ~ DAY_OF_MONTH + YEAR + CRS_DEP_HOUR  + CRS_ARR_TIME 
                 + CRS_ELAPSED_TIME + humidity + precipMM)

#grow tree
fit.tree <- rpart(f1, data = df.train.dt,control = rpart.control(cp = 0.001), method = "anova")
printcp(fit.tree) # display the results
plotcp(fit.tree) # visualize cross-validation results
#A good choice of cp for pruning is often the leftmost value 
#for which the mean lies below the horizontal line."

```

```{r}
# plot tree
rpart.plot(fit.tree, type = 1)
```

```{r}
# prune the tree
optimal_cp <- fit.tree$cptable[which.min(fit.tree$cptable[,"xerror"]),"CP"]
pfit<- prune(fit.tree, cp=optimal_cp) # from cptable   
rpart.plot(pfit)
summary(pfit)

yhat.train.tree <- predict(pfit, df.train.dt) 
mse.train.tree <- mean((y.train.dt - yhat.train.tree) ^ 2)
mse.train.tree

yhat.test.tree <- predict(pfit, df.test.dt) 
mse.test.tree <- mean((y.test.dt - yhat.test.tree) ^ 2)
mse.test.tree

rmse_tree_test <- sqrt(mse.test.tree)
rmse_tree_test 

```

The RSME test using Decision Tree  is `r rmse_tree_test`. We were unable to yield a lower RMSE than the one using Ridge Regression.


#### Random Forrest 

For Random Forrest, we tried different formulas but decided 'f6' yield the lower MSE.

```{r}
smp_size <- floor(.75 * nrow(df.la))
train_index <- sample(nrow(df.la), smp_size) 
df.test.rf <- df.la[-train_index,] ##not the one in train_index
df.train.rf <-df.la[train_index,] 

y.train.rf <- df.train.rf$DEP_DELAY
y.test.rf <- df.test.rf$DEP_DELAY

f6 <- as.formula(DEP_DELAY ~ DAY_OF_MONTH + AIRLINE + CRS_DEP_HOUR  + CRS_ARR_TIME 
                 + CRS_ELAPSED_TIME)
fit.rndfor <- randomForest(f6, df.train.rf,ntree=500, do.trace=F)

print(fit.rndfor) #View results
importance(fit.rndfor)
#We can check which variables are most predictive using a variable importance plot
varImpPlot(fit.rndfor) #Arrival time, CRS Elapsed time, Depature hour, Airline and Day of Month

```


For Random Forrest, the top three predictors are Arrival Time, Elapsed Time and Day of Month.


```{r}
#Calculate the Train MSE
yhat.train.rndfor <- predict(fit.rndfor, df.train.rf) 
mse.train.rndfor <- mean((y.train.rf - yhat.train.rndfor) ^ 2)
mse.train.rndfor

#Calculate the Test MSE
yhat.test.rndfor <- predict(fit.rndfor, df.test.rf) 
mse.test.rndfor <- mean((y.test.rf - yhat.test.rndfor) ^ 2)
mse.test.rndfor

rmse.test.rndfor <- sqrt(mse.test.rndfor)
rmse.test.rndfor
```